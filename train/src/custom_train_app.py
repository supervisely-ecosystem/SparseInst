from supervisely.nn.training.train_app import TrainApp
from supervisely.nn.training.loggers import train_logger
from supervisely import logger
import shutil
from os import listdir
from os.path import basename, isdir, isfile, join
import supervisely.io.fs as sly_fs


class CustomTrainApp(TrainApp):
    def _setup_logger_callbacks(self):
        """
        Set up callbacks for the training logger.
        """
        epoch_pbar = None
        step_pbar = None

        def start_training_callback(total_epochs: int):
            """
            Callback function that is called when the training process starts.
            """
            nonlocal epoch_pbar
            logger.info(f"Training started for {total_epochs} iterations")
            pbar_widget = self.progress_bar_main
            pbar_widget.show()
            epoch_pbar = pbar_widget(message=f"Iterations", total=total_epochs)

        def finish_training_callback():
            """
            Callback function that is called when the training process finishes.
            """
            self.progress_bar_main.hide()
            self.progress_bar_secondary.hide()
            train_logger.close()

        def start_epoch_callback(total_steps: int):
            """
            Callback function that is called when a new epoch starts.
            """
            nonlocal step_pbar
            logger.info(f"Epoch started. Total steps: {total_steps}")
            pbar_widget = self.progress_bar_secondary
            pbar_widget.show()
            step_pbar = pbar_widget(message=f"Steps", total=total_steps)

        def finish_epoch_callback():
            """
            Callback function that is called when an epoch finishes.
            """
            epoch_pbar.update(1)

        def step_callback():
            """
            Callback function that is called when a step iteration is completed.
            """
            step_pbar.update(1)

        train_logger.add_on_train_started_callback(start_training_callback)
        train_logger.add_on_train_finish_callback(finish_training_callback)

        train_logger.add_on_epoch_started_callback(start_epoch_callback)
        train_logger.add_on_epoch_finish_callback(finish_epoch_callback)

        train_logger.add_on_step_finished_callback(step_callback)

    # def _preprocess_artifacts(self, experiment_info: dict) -> None:
    #     """
    #     Preprocesses and move the artifacts generated by the training process to output directories.

    #     :param experiment_info: Information about the experiment results.
    #     :type experiment_info: dict
    #     """
    #     # Preprocess artifacts
    #     logger.debug("Preprocessing artifacts")
    #     if "model_files" not in experiment_info:
    #         experiment_info["model_files"] = {}
    #     else:
    #         # Move model files to output directory except config, config will be processed next
    #         files = {k: v for k, v in experiment_info["model_files"].items() if k != "config"}
    #         for file in files:
    #             if isfile:
    #                 shutil.move(
    #                     file,
    #                     join(self.output_dir, sly_fs.get_file_name_with_ext(file)),
    #                 )
    #             elif isdir:
    #                 shutil.move(file, join(self.output_dir, basename(file)))

    #     # Preprocess config
    #     logger.debug("Preprocessing config")
    #     config = experiment_info["model_files"].get("config")
    #     if config is not None:
    #         experiment_info["model_files"]["config"] = config
    #         if self.is_model_benchmark_enabled:
    #             self._benchmark_params["model_files"]["config"] = config

    #     # Prepare checkpoints
    #     checkpoints = experiment_info["checkpoints"]
    #     # If checkpoints returned as directory
    #     if isinstance(checkpoints, str):
    #         checkpoint_paths = []
    #         for checkpoint_path in listdir(checkpoints):
    #             checkpoint_ext = sly_fs.get_file_ext(checkpoint_path)
    #             if checkpoint_ext in [".pt", ".pth"]:
    #                 checkpoint_paths.append(join(checkpoints, checkpoint_path))
    #     elif isinstance(checkpoints, list):
    #         checkpoint_paths = checkpoints
    #     else:
    #         raise ValueError(
    #             "Checkpoints should be a list of paths or a path to directory with checkpoints"
    #         )

    #     new_checkpoint_paths = []
    #     best_checkpoints_name = experiment_info["best_checkpoint"]
    #     for checkpoint_path in checkpoint_paths:
    #         new_checkpoint_path = join(
    #             self._output_checkpoints_dir,
    #             sly_fs.get_file_name_with_ext(checkpoint_path),
    #         )
    #         shutil.move(checkpoint_path, new_checkpoint_path)
    #         new_checkpoint_paths.append(new_checkpoint_path)
    #         if sly_fs.get_file_name_with_ext(checkpoint_path) == best_checkpoints_name:
    #             experiment_info["best_checkpoint"] = new_checkpoint_path
    #             if self.is_model_benchmark_enabled:
    #                 self._benchmark_params["model_files"]["checkpoint"] = new_checkpoint_path
    #     experiment_info["checkpoints"] = new_checkpoint_paths

    #     # Prepare logs
    #     if sly_fs.dir_exists(self.log_dir):
    #         logs_dir = join(self.output_dir, "logs")
    #         shutil.copytree(self.log_dir, logs_dir)
    #     return experiment_info